{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tablar_nov_2021_(2).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNmYFI/V2zrjc0qJuy9Gq3C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riotamoriya/kaggle/blob/main/tablar_nov_2021_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nilBFOaj9mI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c45112-965f-4b8d-e9a7-7179425765e6"
      },
      "source": [
        "#@title Set the environment\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import csv\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "import sys\n",
        "import os\n",
        "import gc\n",
        "\n",
        "pd.options.display.max_rows = 1000\n",
        "pd.options.display.max_columns = 20\n",
        "\n",
        "from pathlib import Path\n",
        "if 'google.colab' in sys.modules:  # colab環境\n",
        "    INPUT = Path('/content/input/')\n",
        "elif 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    INPUT = Path('../input/')\n",
        "\n",
        "# メタ情報を取得\n",
        "from requests import get\n",
        "name_notebook = get('http://172.28.0.2:9000/api/sessions').json()[0]['name']\n",
        "print(name_notebook)\n",
        "\n",
        "# google drive と接続\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# LINE 通知機能読み込み\n",
        "from drive.MyDrive.Kaggle_emwa.myPythonLib import send_line_notify as sln\n",
        "# 実行時間計測機能\n",
        "from drive.MyDrive.Kaggle_emwa.myPythonLib import MEASURE_RUN_TIME\n",
        "mrt = MEASURE_RUN_TIME()\n",
        "\n",
        "\n",
        "# kaggleと接続\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/Kaggle_emwa/kaggle.json ~/.kaggle\n",
        "!pip install kaggle\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "# !kaggle competitions list"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tablar_nov_2021_(2).ipynb\n",
            "Mounted at /content/drive\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=ca0d5740d73ce831c9a3aa49b9c9fdcb59a0855804db69c80dfb48fb7ed00fdf\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc5QCdtcqOPc",
        "outputId": "e0a94eea-966f-4ccc-d890-25f443b69aff"
      },
      "source": [
        "tryproblem = 'tabular-playground-series-nov-2021'\n",
        "tryproblem += '.zip'\n",
        "\n",
        "if not os.path.exists(tryproblem):\n",
        "  !kaggle competitions download -c tabular-playground-series-nov-2021\n",
        "  import zipfile\n",
        "  with zipfile.ZipFile(tryproblem) as existing_zip:\n",
        "      existing_zip.extractall('.')\n",
        "row_train = pd.read_csv('./train.csv')\n",
        "row_test = pd.read_csv('./test.csv')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading tabular-playground-series-nov-2021.zip to /content\n",
            " 99% 425M/428M [00:04<00:00, 73.0MB/s]\n",
            "100% 428M/428M [00:04<00:00, 93.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9RXQ67Bx7EN"
      },
      "source": [
        "from __future__ import print_function\n",
        "from tensorflow.python import keras\n",
        "from tensorflow.python.keras.datasets import mnist\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "## Import Necessary Modules\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "class Mish(Activation):\n",
        "    def __init__(self, activation, **kwargs):\n",
        "        super(Mish, self).__init__(activation, **kwargs)\n",
        "        self.__name__ = 'Mish'\n",
        "\n",
        "def mish(inputs):\n",
        "    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
        "\n",
        "get_custom_objects().update({'Mish': Mish(mish)})"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNVyMRLg_YWG"
      },
      "source": [
        "train = row_train.drop(['id','target'], axis = 1)\n",
        "test = row_test.drop('id', axis = 1)\n",
        "\n",
        "target = row_train.target\n",
        "id_train = row_train.id\n",
        "id_test = row_test.id\n",
        "\n",
        "# ハイパーパラメータ定義\n",
        "EPOCHS = 700\n",
        "BATCH_SIZE = 2048 \n",
        "ACTIVATION = 'Mish'\n",
        "LEARNING_RATE = 0.0007\n",
        "FOLDS = 5"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_nKIwd5AqQU"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "# the number 2 is just a threshold to split \n",
        "# 歪度２を基準に、歪んだ分布と歪んでない分布とで２分割する\n",
        "# 事前に行なったEDAによって、単峰性と二峰性とで大別されている\n",
        "h_skew = train.loc[:,train.skew() >= 2].columns  # with Skewed \n",
        "l_skew = train.loc[:,train.skew() < 2].columns   # Bimodal\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "### 単峰性分布の特徴量に対して\n",
        "\n",
        "# train['mean_h'] = train[h_skew].mean(axis=1)\n",
        "# test['mean_h'] = test[h_skew].mean(axis=1)\n",
        "\n",
        "# train['std_h'] = np.log1p(train[h_skew].std(axis=1))\n",
        "# test['std_h'] = np.log1p(test[h_skew].std(axis=1))\n",
        "\n",
        "# 中央値を取得\n",
        "train['median_h'] = train[h_skew].median(axis=1)\n",
        "test['median_h'] = test[h_skew].median(axis=1)\n",
        "\n",
        "# train['min_h'] = train[h_skew].min(axis=1)\n",
        "# test['min_h'] = test[h_skew].min(axis=1)\n",
        "\n",
        "# train['skew_h'] = train[h_skew].skew(axis=1)\n",
        "# test['skew_h'] = test[h_skew].skew(axis=1)\n",
        "\n",
        "# train['max_h'] = train[h_skew].max(axis=1)\n",
        "# test['max_h'] = test[h_skew].max(axis=1)\n",
        "\n",
        "# 分散値を取得\n",
        "train['var_h'] = train[h_skew].var(axis=1)\n",
        "test['var_h'] = test[h_skew].var(axis=1)\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Bimodal distributions\n",
        "### 二峰性分布の特徴量に対して\n",
        "\n",
        "# 平均値\n",
        "train['mean_l'] = train[l_skew].mean(axis=1)\n",
        "test['mean_l'] = test[l_skew].mean(axis=1)\n",
        "\n",
        "# 標準偏差\n",
        "train['std_l'] = train[l_skew].std(axis=1)\n",
        "test['std_l'] = test[l_skew].std(axis=1)\n",
        "\n",
        "# 中央値\n",
        "train['median_l'] = train[l_skew].median(axis=1)\n",
        "test['median_l'] = test[l_skew].median(axis=1)\n",
        "\n",
        "# train['min_l'] = train[l_skew].min(axis=1)\n",
        "# test['min_l'] = test[l_skew].min(axis=1)\n",
        "\n",
        "# 歪度\n",
        "train['skew_l'] = train[l_skew].skew(axis=1)\n",
        "test['skew_l'] = test[l_skew].skew(axis=1)\n",
        "\n",
        "# 最大値\n",
        "train['max_l'] = train[l_skew].max(axis=1)\n",
        "test['max_l'] = test[l_skew].max(axis=1)\n",
        "\n",
        "# 分散値\n",
        "train['var_l'] = train[l_skew].var(axis=1)\n",
        "test['var_l'] = test[l_skew].var(axis=1)\n",
        "\n",
        "\n",
        "raw_train = train.copy()\n",
        "raw_test = test.copy()\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Scaling and Nomalization\n",
        "# Min-Maxスケーリング\n",
        "transformer_high_skew = make_pipeline(\n",
        "    StandardScaler(), \n",
        "    MinMaxScaler(feature_range=(0, 1))\n",
        ")\n",
        "\n",
        "transformer_low_skew = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    MinMaxScaler(feature_range=(0, 1))\n",
        ")\n",
        "\n",
        "# 追加した特徴量（統計量）\n",
        "new_cols = train.columns[-8:]\n",
        "\n",
        "transformer_new_cols = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    MinMaxScaler(feature_range=(0, 1))\n",
        ")\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "    (transformer_high_skew, l_skew),\n",
        "    (transformer_low_skew, h_skew),\n",
        "    (transformer_new_cols, new_cols),\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B-8Qym2At4f"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Seed \n",
        "\n",
        "my_seed = 42\n",
        "def seedAll(seed):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    \n",
        "seedAll(my_seed)\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "def load_model(name:str):\n",
        "    \n",
        "    early_stopping = callbacks.EarlyStopping( # アーリーストッピングの定義\n",
        "        monitor='val_loss', # 基準の値\n",
        "        min_delta=0, # val_loss が 0 よりも変化が小さければ、改善していないと判断\n",
        "        patience=20, # 20回までは確実に訓練するが、それまでに改善がなければ停止する\n",
        "        verbose=0, \n",
        "        mode='min', # val_loss の減少が停止した時に、訓練を停止\n",
        "        restore_best_weights=True,\n",
        "        baseline=None,\n",
        "    )\n",
        "\n",
        "    plateau = callbacks.ReduceLROnPlateau( # 動的に学習率を変える\n",
        "            monitor='val_loss', # 基準の値\n",
        "            factor=0.2, # どのくらい減らすのか\n",
        "            patience=7, # ７エポックの間に評価値が改善しなければ、学習率を減らす\n",
        "            verbose=0, # 学習率を減らした時に表示する\n",
        "            mode='min' # val_loss の減少が停止したとき\n",
        "            )\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Model \n",
        "    \n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(108, activation = ACTIVATION, input_shape = [train.shape[1]]),      \n",
        "        layers.Dense(64, activation =ACTIVATION), \n",
        "        layers.Dense(32, activation =ACTIVATION),\n",
        "        layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        " \n",
        "    model.compile(\n",
        "        optimizer= keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['AUC'],\n",
        "    )\n",
        "    \n",
        "    return model, early_stopping, plateau"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia9zPSjK-TGX"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Seed \n",
        "\n",
        "my_seed = 42\n",
        "def seedAll(seed):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    \n",
        "seedAll(my_seed)\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "def load_model(name:str):\n",
        "    \n",
        "    early_stopping = callbacks.EarlyStopping(\n",
        "        patience=20,\n",
        "        min_delta=0,\n",
        "        monitor='val_loss',\n",
        "        restore_best_weights=True,\n",
        "        verbose=0,\n",
        "        mode='min', \n",
        "        baseline=None,\n",
        "    )\n",
        "\n",
        "    plateau = callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss', \n",
        "            factor=0.2, \n",
        "            patience=7, \n",
        "            verbose=0,\n",
        "            mode='min')\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Model \n",
        "\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(108, activation = ACTIVATION, input_shape = [train.shape[1]]),      \n",
        "        layers.Dense(64, activation =ACTIVATION), \n",
        "        layers.Dense(32, activation =ACTIVATION),\n",
        "        layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "    model.compile(\n",
        "        optimizer= keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['AUC'],\n",
        "    )\n",
        "    \n",
        "    return model, early_stopping, plateau"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpyeqY1KAVeq"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, log_loss\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\n",
        "\n",
        "preds_valid_f = {}\n",
        "preds_test = []\n",
        "total_auc = []\n",
        "f_scores = []\n",
        "\n",
        "kf = StratifiedKFold(n_splits=FOLDS,random_state=0,shuffle=True)\n",
        "\n",
        "for fold,(train_index, valid_index) in enumerate(kf.split(train,target)):\n",
        "\n",
        "    X_train,X_valid = train.loc[train_index], train.loc[valid_index]\n",
        "    y_train,y_valid = target.loc[train_index], target.loc[valid_index]\n",
        "\n",
        "    #   --------------------------------------------------------  \n",
        "    # Preprocessing\n",
        "    index_valid  = X_valid.index.tolist()\n",
        "    test  = raw_test.copy()\n",
        "    \n",
        "    X_train = preprocessor.fit_transform(X_train)\n",
        "    X_valid = preprocessor.transform(X_valid)\n",
        "    test = preprocessor.transform(test)\n",
        "    \n",
        "    #  ----------------------------------------------------------    \n",
        "    # Model\n",
        "    \n",
        "    model, early_stopping, plateau  = load_model('version1')\n",
        "\n",
        "    history = model.fit(  X_train, y_train,\n",
        "                validation_data = (X_valid, y_valid),\n",
        "                batch_size = BATCH_SIZE, \n",
        "                epochs = EPOCHS,\n",
        "                callbacks = [early_stopping, plateau],\n",
        "                shuffle = True,\n",
        "                verbose = 0\n",
        "              )\n",
        "\n",
        "    #  ----------------------------------------------------------\n",
        "    #  oof\n",
        "    preds_valid = model.predict(X_valid).reshape(1,-1)[0] \n",
        "    \n",
        "    #  ----------------------------------------------------------\n",
        "    #  test  predictions\n",
        "    preds_test.append(model.predict(test).reshape(1,-1)[0])\n",
        "    \n",
        "    #  ----------------------------------------------------------\n",
        "    #  Saving  scores to plot the end  \n",
        "    scores = pd.DataFrame(history.history)\n",
        "    scores['folds'] = fold\n",
        "    \n",
        "    if fold == 0:\n",
        "        f_scores = scores \n",
        "    else: \n",
        "        f_scores = pd.concat([f_scores, scores], axis  = 0)\n",
        "        \n",
        "    #  ----------------------------------------------------------\n",
        "    #  concatenating valid preds\n",
        "    preds_valid_f.update(dict(zip(index_valid, preds_valid)))\n",
        "\n",
        "    # Getting score for a fold model\n",
        "    fold_auc = roc_auc_score(y_valid, preds_valid)\n",
        "    print(f\"Fold {fold} roc_auc_score: {fold_auc}\")\n",
        "\n",
        "    # Total auc\n",
        "    total_auc.append(fold_auc)\n",
        "\n",
        "print(f\"mean roc_auc_score: {np.mean(total_auc)}, std: {np.std(total_auc)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14TtjZ7OsA4M"
      },
      "source": [
        "sln(\"fin mish\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0qIJWVVDRQS"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for fold in range(f_scores['folds'].nunique()):\n",
        "    history_f = f_scores[f_scores['folds'] == fold]\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(14,4))\n",
        "    fig.suptitle('Fold : '+str(fold), fontsize=14)\n",
        "        \n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(history_f.loc[:, ['loss', 'val_loss']], label= ['loss', 'val_loss'])\n",
        "    plt.legend(fontsize=15)\n",
        "    plt.grid()\n",
        "    \n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history_f.loc[:, ['auc', 'val_auc']],label= ['auc', 'val_auc'])\n",
        "    plt.legend(fontsize=15)\n",
        "    plt.grid()\n",
        "    \n",
        "    print(\"Validation Loss: {:0.4f}\".format(history_f['val_loss'].min()));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "2DDMJvDEDWA4",
        "outputId": "32210727-e365-4887-c4d5-4c48fe977286"
      },
      "source": [
        "sub = pd.read_csv(\"./sample_submission.csv\")\n",
        "sub['target'] = np.mean(preds_test, axis = 0)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "sub.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>600000</td>\n",
              "      <td>0.740264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>600001</td>\n",
              "      <td>0.752065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>600002</td>\n",
              "      <td>0.749946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>600003</td>\n",
              "      <td>0.389264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>600004</td>\n",
              "      <td>0.722208</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id    target\n",
              "0  600000  0.740264\n",
              "1  600001  0.752065\n",
              "2  600002  0.749946\n",
              "3  600003  0.389264\n",
              "4  600004  0.722208"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt7mavEmD1T-",
        "outputId": "7fe71029-e0dc-4ea9-cb87-741cb480f951"
      },
      "source": [
        "!kaggle competitions submit -c tabular-playground-series-nov-2021 -f submission.csv -m '4 mish try'\n",
        "sln(\"finish to submit\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 9.17M/9.17M [00:01<00:00, 6.86MB/s]\n",
            "Successfully submitted to Tabular Playground Series - Nov 2021"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c48lveskETGJ"
      },
      "source": [
        "Ref:\n",
        "https://www.kaggle.com/javiervallejos/simple-nn-with-good-results-tps-nov-21"
      ]
    }
  ]
}